# @package optimizer
name: Adam
epochs: 100

lr: 1e-2 # Learning rate
weight_decay: 5e-4 # Regularization
label_smoothing: 0.0

# LR Scheduler (step)
decay_frequency: 25000
decay_factor: 0.1
warmup_steps: 0

training_multiplier: 1 # Useful for RigL 5x etc.