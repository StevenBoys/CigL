# @package optimizer
name: SGD
epochs: 100

lr: 0.1 # Learning rate
momentum: 0.9 # SGD momentum
weight_decay: 5e-4 # Regularization
label_smoothing: 0.0

# LR Scheduler
decay_frequency: 25000
decay_factor: 0.1
warmup_steps: 0

use_nesterov: True

training_multiplier: 1 # Useful for RigL 5x etc.